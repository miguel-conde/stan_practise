---
title: Problem 16.1 Discoveries data revisited 
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The file evaluation_discoveries.csv contains data on the numbers of “great” inventions and
scientific discoveries ($X_t$) in each year from 1860 to 1959. In this question you will develop a
model to explain the variation in scientific inventions over time. The simplest model here is to
assume that 

1. one discovery is independent of all others, and 
2. the rate of occurrence of
discoveries is the same in all years ($\lambda$). 

Since the data is discrete, these assumptions suggest the
use a Poisson likelihood, $X_t ∼ Poisson(\lambda)$.

# Problem 16.1.1. 

Open a text editor and create a file called “discoveries.stan” in your working
directory. In the file create three parameter blocks:

```
data {

}

parameters {

}

model {

}
```

# Problem 16.1.2. 

Fill in the data and parameter blocks for the above model.

# Problem 16.1.3 

Using a $log-N( 2,1)$ prior for $\lambda$, code up the model block, making sure to save your file afterwards.

# Problem 16.1.4 

Open your statistical software (R, Python, Matlab, and so on) and load any packages necessary to use Stan. (Hint: in R this is done by using library( rstan); in Python this is done using import pystan.)

```{r}
library(rstan)
library(tidyverse)
```


# Problem 16.1.5 

Load the data into your software and then put it into a structure that can be passed to Stan. (Hint: in R create a list of the data; in Python create a dictionary where the ‘key’ for each variable is the desired variable name.)

```{r}
eval_disc <- readr::read_csv(here::here("STAN", "data", "evaluation_discoveries.csv"))

X <- eval_disc$discoveries
N <- length(X)
```


# Problem 16.1.6

Run your model using Stan, with four chains, each with a sample size of 1000, and a warm-up of 500 samples. Set seed = 1 to allow for reproducibility of your results. Store your result in an object called fit.

```{r}
options(mc.cores = parallel:: detectCores())
rstan_options(auto_write = TRUE)

set.seed = 1

fit <- stan(here::here("STAN", "problem_16_1", "discoveries.stan"), 
            data = list(N = N, X = X), 
            iter = 1000, 
            chains = 4, 
            warmup = 500)
```


# Problem 16.1.7 

Diagnose whether your model has converged by printing fit.

```{r}
# Lambda and lp should both have a value of Rˆ ≈ 1.
print(fit)
```


$lambda$ and $lp$ should both have a value of $\hat{R} \approx 1$.

# Problem 16.1.8 

For your sample what is the equivalent number of samples for an independent sampler?

This is just the value of “n_eff” which I get to be around 600-1066.

# Problem 16.1.9 

Find the central posterior 80% credible interval for $\lambda$.

```{r}
lambda_post <- rstan::extract(fit, 'lambda')[[1]]

# Find the central posterior 80% credible interval for λ.
quantile(lambda_post, probs = c(0.1, 0.9))

print(fit, pars='lambda', probs = c(0.1, 0.9))
```


# Problem 16.1.10 

Draw a histogram of your posterior samples for $\lambda$.

```{r}
qplot(lambda_post)
```


# Problem 16.1.11 

Load the evaluation_discoveries.csv data and graph it. What does this suggest about our model’s assumptions?

```{r}
plot(eval_disc, type = "o")
```


# Problem 16.1.12 

Create a generated quantities block in your Stan file, and use it to sample from the posterior predictive distribution. Then carry out appropriate posterior predictive checks to evaluate your model. (Hint: use the poisson_rng function to generate independent samples from your lambda.)

One simple check is to compare the maximum of your posterior predictive 
simulations with that of the real data (which is 12 discoveries in 1885)

```{r}
lXSim <- rstan::extract(fit, 'XSim')[[1]]
lMax <- apply(lXSim, 1, max)

qplot(lMax)
sum(lMax >= 12) / length(lMax)
```

